Data compression~Entropy and information~Information theory~Statistical randomness~
||||||
Entropy (information theory)
||||||
In information theory, the entropy of a random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes. Given a discrete random variable 
  
    
      
        X
      
    
    {\displaystyle X}
  , which takes values in the alphabet 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   and is distributed according to 
  
    
      
        p
        :
        
          
            X
          
        
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle p:{\mathcal {X}}\to [0,1]}
  :

where 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   denotes the sum over the variable's possible values. The choice of base for 
  
    
      
        log
      
    
    {\displaystyle \log }
  , the logarithm, varies for different applications. Base 2 gives the unit of bits (or "shannons"), while base e gives "natural units" nat, and base 10 gives units of "dits", "bans", or "hartleys". An equivalent definition of entropy is the expected value of the self-information of a variable.
The concept of information entropy was introduced by Claude Shannon in his 1948 paper "A Mathematical Theory of Communication", and is also referred to as Shannon entropy. Shannon's theory defines a data communication system composed of three elements: a source of data, a communication channel, and a receiver. The "fundamental problem of communication" – as expressed by Shannon – is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. Shannon considered various ways to encode, compress, and transmit messages from a data source, and proved in his famous source coding theorem that the entropy represents an absolute mathematical limit on how well data from the source can be losslessly compressed onto a perfectly noiseless channel. Shannon strengthened this result considerably for noisy channels in his noisy-channel coding theorem.
Entropy in information theory is directly analogous to the entropy in statistical thermodynamics. The analogy results when the values of the random variable designate energies of microstates, so Gibbs formula for the entropy is formally identical to Shannon's formula. Entropy has  relevance to other areas of mathematics such as combinatorics and machine learning. The definition can be derived from a set of axioms establishing that entropy should be a measure of how "surprising" the average outcome of a variable is. For a continuous random variable, differential entropy is analogous to entropy.